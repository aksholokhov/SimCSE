{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "18939441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"MODEL_DIR\"] = '../model'\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "import nlpaug.augmenter.word as naw\n",
    "import numpy as np\n",
    "from simcse import SimCSE\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "80e07896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "csv\t\t    wiki1m_for_simcse.txt  wiki_delete_one_word.csv\r\n",
      "download_nli.sh     wiki_cropping_0.1.csv  wiki_synonym_replacement.csv\r\n",
      "download_wiki.sh    wiki_cropping_0.1.txt  wiki_word_deletion_0.1.csv\r\n",
      "nli_for_simcse.csv  wiki_cropping_0.2.csv  wiki_word_deletion_0.2.csv\r\n",
      "text\t\t    wiki_cropping_0.3.csv  wiki_word_deletion_0.3.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "08846975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/05/2023 12:09:19 - WARNING - datasets.builder -   Using custom data configuration default-e985b51e47473734\n",
      "03/05/2023 12:09:19 - WARNING - datasets.builder -   Found cached dataset text (/home/aksh/SimCSE/cse517wi23_reproducibility_study/../data/text/default-e985b51e47473734/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3cc78c8f63c4ec5a84721fba1fe0cac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_folder = Path('../data/').resolve()\n",
    "data_files = {\n",
    "    \"train\": str(data_folder / \"wiki1m_for_simcse.txt\")\n",
    "}\n",
    "extension = \"text\"\n",
    "\n",
    "wiki_dataset = load_dataset(extension, data_files=data_files, cache_dir=\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6c7e705a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e9ee6947dd423bb01649818437fa74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cropping:   0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a02d3219a93042269617bd4137ad2eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1000 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc949c770d04b0f8237c007e9f37c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cropping:   0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2a67995ecc44cca94ad527316cdb0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1000 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b157156894c54f0b85a29c267b2d0e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "cropping:   0%|          | 0/1000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67efde87136d4fc681b4acdfb2667a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1000 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aug = naw.SynonymAug(aug_src='wordnet')\n",
    "rnd = np.random.default_rng(42)\n",
    "\n",
    "\n",
    "def crop_sentence(sentence, fraction=0.1):\n",
    "    return sentence[:int((1-fraction)*len(sentence))]\n",
    "\n",
    "def delete_words(sentence, fraction=0.1):\n",
    "    words = sentence.split(\" \")\n",
    "    n_words = len(words)\n",
    "    n_words_to_keep = int((1 - fraction)*len(words))\n",
    "    ids = sorted(rnd.choice(range(n_words), n_words_to_keep, replace=False))\n",
    "    return \" \".join([words[i] for i in ids])\n",
    "\n",
    "def delete_one_word(sentence):\n",
    "    words = sentence.split(\" \")\n",
    "    idx_delete = rnd.integers(len(words))\n",
    "    del words[idx_delete]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def replace_a_word_with_synonym(sentence):\n",
    "    return aug.augment(sentence)\n",
    "    \n",
    "for augmentation, kwargs in [\n",
    "    (\"cropping\", {\"fraction\": 0.1}),\n",
    "    (\"cropping\", {\"fraction\": 0.2}),\n",
    "    (\"cropping\", {\"fraction\": 0.3}),\n",
    "#     (\"word_deletion\", {\"fraction\": 0.1}),\n",
    "#     (\"word_deletion\", {\"fraction\": 0.2}),\n",
    "#     (\"word_deletion\", {\"fraction\": 0.3}),\n",
    "#     (\"delete_one_word\", {}),\n",
    "#     (\"synonym_replacement\", {})\n",
    "]:\n",
    "    sent1 = []\n",
    "    sent2 = []\n",
    "    for idx, sentence in enumerate(tqdm(wiki_dataset['train'], desc=f'{augmentation}')):\n",
    "        if augmentation == \"cropping\":\n",
    "            new_sentence = crop_sentence(sentence['text'], **kwargs)\n",
    "        elif augmentation == \"word_deletion\":\n",
    "            new_sentence = delete_words(sentence['text'], **kwargs)\n",
    "        elif augmentation == \"delete_one_word\":\n",
    "            new_sentence = delete_one_word(sentence['text'], **kwargs)\n",
    "        elif augmentation == \"synonym_replacement\":\n",
    "            new_sentence = replace_a_word_with_synonym(sentence['text'], **kwargs)[0]\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unknown augmentation: {augmentation}\")\n",
    "        sent1.append(sentence['text'])\n",
    "        sent2.append(new_sentence)\n",
    "    new_dataset = Dataset.from_dict({'sent1': sent1, 'sent2': sent2})\n",
    "    suffix = f'_{kwargs[\"fraction\"]}' if \"fraction\" in kwargs else \"\"\n",
    "    new_dataset.to_csv(data_folder / f'wiki_{augmentation}{suffix}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "82652ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/06/2023 09:13:39 - WARNING - datasets.builder -   Using custom data configuration default-9ebaf222e0a0f84f\n",
      "03/06/2023 09:13:39 - WARNING - datasets.builder -   Found cached dataset csv (/home/aksh/SimCSE/cse517wi23_reproducibility_study/../data/csv/default-9ebaf222e0a0f84f/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d884a53fa91449b870e8988f4ce7188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_folder = Path('../data/').resolve()\n",
    "data_files = {\n",
    "    \"train\": str(data_folder / \"wiki_cropping_0.1.csv\")\n",
    "}\n",
    "extension = \"csv\"\n",
    "\n",
    "wiki_dataset = load_dataset(extension, data_files=data_files, cache_dir=\"../data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c98001",
   "metadata": {},
   "source": [
    "## Alignment and uniformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1deb1113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from senteval.sts import STSBenchmarkEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f2ce1bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sts_b_path = Path(\"../SentEval/data/downstream/STS/STSBenchmark/\")\n",
    "sts_b_dataset = STSBenchmarkEval(sts_b_path)\n",
    "sufficiently_close_sent1 = []\n",
    "sufficiently_close_sent2 = []\n",
    "all_sentences_1 = []\n",
    "all_sentences_2 = []\n",
    "gs_scores = []\n",
    "for dataset in ['train', 'dev', 'test']:\n",
    "    for sent1, sent2, score in zip(*sts_b_dataset.data[dataset]):\n",
    "        if score > 4:\n",
    "            sufficiently_close_sent1.append(\" \".join(sent1))\n",
    "            sufficiently_close_sent2.append(\" \".join(sent2))\n",
    "        all_sentences_1.append(\" \".join(sent1))\n",
    "        all_sentences_2.append(\" \".join(sent2))\n",
    "        gs_scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "385fda08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def aligment(embeddings_1, embeddings_2): \n",
    "#     return torch.mean(torch.sum(torch.square(embeddings_1 - embeddings_2), dim=-1)).item()\n",
    "\n",
    "# def uniformity(embeddings_1, embeddings_2):\n",
    "#     return torch.log(torch.mean(torch.exp(-2*torch.sum(torch.square(embeddings_1 - embeddings_2), dim=-1)))).item()\n",
    "\n",
    "def _norm(x, eps=1e-8): \n",
    "    xnorm = torch.linalg.norm(x, dim=-1)\n",
    "    xnorm = torch.max(xnorm, torch.ones_like(xnorm) * eps)\n",
    "    return x / xnorm.unsqueeze(dim=-1)\n",
    "\n",
    "# from Wang and Isola (with a bit of modification)\n",
    "# only consider pairs with gs > 4 (from footnote 3)\n",
    "def _lalign(x, y, ok, alpha=2):\n",
    "    return ((_norm(x) - _norm(y)).norm(dim=1).pow(alpha) * ok).sum() / ok.sum()\n",
    "\n",
    "def _lunif(x, t=2):\n",
    "    sq_pdist = torch.pdist(_norm(x), p=2).pow(2)\n",
    "    return sq_pdist.mul(-t).exp().mean().log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e9fe6999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "delete_one_word\t\t\t   wiki_cropping_0.1  wiki_word_deletion_0.1\r\n",
      "my-unsup-simcse-bert-base-uncased  wiki_cropping_0.2  wiki_word_deletion_0.2\r\n",
      "synonym_replacement\t\t   wiki_cropping_0.3  wiki_word_deletion_0.3\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../result/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8eb3d4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ../result/unsup_simcse and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "03/03/2023 17:35:01 - INFO - simcse.tool -   Use `cls_before_pooler` for unsupervised models. If you want to use other pooling policy, specify `pooler` argument.\n",
      "100%|███████████████████████████████████████████████████████████████████████| 135/135 [00:07<00:00, 17.82it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 135/135 [00:07<00:00, 18.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "align 0.19683097302913666\t\t uniform -2.444230318069458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ../result/delete_one_word and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|███████████████████████████████████████████████████████████████████████| 135/135 [00:07<00:00, 17.71it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 135/135 [00:07<00:00, 18.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "align 0.21235987544059753\t\t uniform -2.378030300140381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ../result/synonym_replacement and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|███████████████████████████████████████████████████████████████████████| 135/135 [00:07<00:00, 17.66it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 135/135 [00:07<00:00, 18.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "align 0.18112090229988098\t\t uniform -2.0485191345214844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ../result/wiki_cropping_0.1 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|███████████████████████████████████████████████████████████████████████| 135/135 [00:07<00:00, 17.57it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 135/135 [00:07<00:00, 17.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "align 0.2761279344558716\t\t uniform -2.503908634185791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ../result/wiki_cropping_0.2 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|███████████████████████████████████████████████████████████████████████| 135/135 [00:07<00:00, 17.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 135/135 [00:07<00:00, 17.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "align 0.2948911190032959\t\t uniform -2.515211582183838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ../result/wiki_cropping_0.3 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|███████████████████████████████████████████████████████████████████████| 135/135 [00:07<00:00, 17.50it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 135/135 [00:07<00:00, 17.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "align 0.31880515813827515\t\t uniform -2.5550127029418945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ../result/wiki_word_deletion_0.1 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|███████████████████████████████████████████████████████████████████████| 135/135 [00:07<00:00, 17.35it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 135/135 [00:07<00:00, 17.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "align 0.20204579830169678\t\t uniform -2.447098731994629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ../result/wiki_word_deletion_0.2 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|███████████████████████████████████████████████████████████████████████| 135/135 [00:07<00:00, 17.21it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 135/135 [00:07<00:00, 17.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "align 0.18055179715156555\t\t uniform -2.2859609127044678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at ../result/wiki_word_deletion_0.3 and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 135/135 [00:07<00:00, 17.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 135/135 [00:07<00:00, 17.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "align 0.17033952474594116\t\t uniform -2.0445778369903564\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for model_name in [\n",
    "    \"unsup_simcse\",\n",
    "    \"delete_one_word\",\n",
    "    \"synonym_replacement\",\n",
    "    \"wiki_cropping_0.1\",\n",
    "    \"wiki_cropping_0.2\",\n",
    "    \"wiki_cropping_0.3\",\n",
    "    \"wiki_word_deletion_0.1\",\n",
    "    \"wiki_word_deletion_0.2\",\n",
    "    \"wiki_word_deletion_0.3\"\n",
    "]:\n",
    "    model = SimCSE(str(Path(\"../result\") / model_name))\n",
    "    all_embeddings_1 = model.encode(all_sentences_1)\n",
    "    all_embeddings_2 = model.encode(all_sentences_2)\n",
    "    \n",
    "    ok = (torch.Tensor(gs_scores) > 4).int()\n",
    "    align = _lalign(\n",
    "        all_embeddings_1, \n",
    "        all_embeddings_2, \n",
    "        ok).item()\n",
    "\n",
    "    # consider all sentences (from footnote 3)\n",
    "    unif = _lunif(torch.vstack([all_embeddings_1, all_embeddings_2])).item()\n",
    "    results[model_name] = (align, unif)\n",
    "    print(f'align {align}\\t\\t uniform {unif}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e05ef55c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7785198092460632"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniformity(all_embeddings_1, all_embeddings_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merl",
   "language": "python",
   "name": "merl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
